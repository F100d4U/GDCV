{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GDCV12] 직접 만들어보는 OCR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) End-to-End OCR\n",
    "\n",
    "2) 루브릭\n",
    "\n",
    "3) 회고"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) End-to-End OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lmdb\n",
      "  Downloading lmdb-1.4.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.9/305.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lmdb\n",
      "Successfully installed lmdb-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 14:05:27.168322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:05:27.459384: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-19 14:05:28.297856: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.7/lib64\n",
      "2022-12-19 14:05:28.297940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.7/lib64\n",
      "2022-12-19 14:05:28.297946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import keras_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "DATA_DIR = '/home/june/Github/GDCV/data/gd12'\n",
    "\n",
    "# MJ 데이터셋 위치\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIR, '/MJ_train')\n",
    "VALID_DATA_PATH = os.path.join(DATA_DIR, '/MJ_valid')\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIR, '/MJ_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env에 데이터를 불러올게요\n",
    "# lmdb에서 데이터를 불러올 때 env라는 변수명을 사용하는게 일반적이에요\n",
    "env = lmdb.open(TRAIN_DATA_PATH, \n",
    "                max_readers=32, \n",
    "                readonly=True, \n",
    "                lock=False, \n",
    "                readahead=False, \n",
    "                meminit=False)\n",
    "\n",
    "# 불러온 데이터를 txn(transaction)이라는 변수를 통해 엽니다\n",
    "# 이제 txn변수를 통해 직접 데이터에 접근 할 수 있어요\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        print(f\"━━━━━━━━ {index} ━━━━━━━━\")\n",
    "        # index를 이용해서 라벨 키와 이미지 키를 만들면\n",
    "        # txn에서 라벨과 이미지를 읽어올 수 있어요\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # 이미지는 버퍼를 통해 읽어오기 때문에 \n",
    "        # 버퍼에서 이미지로 변환하는 과정이 다시 필요해요\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "\n",
    "        # 원본 이미지 크기를 출력해 봅니다\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        # 이미지 비율을 유지하면서 높이를 32로 바꿀거에요\n",
    "        # 하지만 너비를 100보다는 작게하고 싶어요\n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32)        \n",
    "        print('target_img_size:{}'.format(target_img_size))        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "\n",
    "        # 이제 높이가 32로 일정한 이미지와 라벨을 함께 출력할 수 있어요       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MJDatasetSequence(Sequence):\n",
    "    # 객체를 초기화 할 때 lmdb를 열어 env에 준비해둡니다\n",
    "    # 또, lmdb에 있는 데이터 수를 미리 파악해둡니다\n",
    "    def __init__(self, \n",
    "                dataset_path,\n",
    "                label_converter,\n",
    "                batch_size=1,\n",
    "                img_size=(100,32),\n",
    "                max_text_len=22,\n",
    "                is_train=False,\n",
    "                character='') :\n",
    "        \n",
    "        self.label_converter = label_converter  # 문자를 미리 정의된 index로 변환해주는 converter\n",
    "        self.batch_size = batch_size  # 배치 사이즈\n",
    "        self.img_size = img_size  # 입력 이미지 크기\n",
    "        self.max_text_len = max_text_len  # 최대 글자 수\n",
    "        self.character = character  # 학습 대상으로 한정하기 위한 character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    # index에 해당하는 image와 label을 읽어옵니다\n",
    "    # 위에서 사용한 코드와 매우 유사합니다\n",
    "    # label을 조금 더 다듬는 것이 약간 다릅니다\n",
    "\n",
    "    # - 이미지 데이터를 img, label 의 쌍으로 가져오는 메서드\n",
    "    # => 다양한 사이즈의 이미지를 모두 height 는 32, width 는 최대 100 이 되도록 가공\n",
    "    def _get_img_label(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width, self.img_size[1])\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            # label을 약간 더 다듬습니다\n",
    "            label = label.upper()\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "            label = label[:self.max_text_len]\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    # __getitem__은 약속되어있는 메서드입니다\n",
    "    # 이 부분을 작성하면 slice할 수 있습니다\n",
    "    # 자세히 알고 싶다면 아래 문서를 참고하세요\n",
    "    # https://docs.python.org/3/reference/datamodel.html#object.__getitem__\n",
    "\n",
    "    # - model.fit() 에서 호출되는 메서드: 배치 단위만큼 _get_img_label()을 통해 가져온 데이터셋 리턴\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. idx에 해당하는 index_list만큼 데이터를 불러\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64') * self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        # 2. image와 label을 불러오고 \n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            # 인코딩 과정에서 '-'이 추가되면 max_text_len보다 길어질 수 있어요\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "        \n",
    "        # 3. 사용하기 좋은 inputs과 outputs형태로 반환합니다\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelConverter(object):\n",
    "\n",
    "    # 입력으로 받은 text를 self.dict 에 각 character 들이 어떤 index 에 매핑되는지 저장\n",
    "    # => character, index 를 통해 모델이 학습할 수 있는 output 이 생성됨\n",
    "    def __init__(self, character):\n",
    "        self.character = \"-\" + character\n",
    "        self.label_map = dict()\n",
    "        for i, char in enumerate(self.character):\n",
    "            self.label_map[char] = i\n",
    "\n",
    "    # character -> index 변환\n",
    "    # cf. 공백 문자를 뜻하는 '-' 의 label 은 0 으로 지정\n",
    "    def encode(self, text):\n",
    "        encoded_label = []\n",
    "        for i, char in enumerate(text):\n",
    "            if i > 0 and char == text[i - 1]:\n",
    "                encoded_label.append(0)    # 같은 문자 사이에 공백(blank) 문자 label을 삽입\n",
    "            encoded_label.append(self.label_map[char])\n",
    "\n",
    "        return np.array(encoded_label)\n",
    "\n",
    "    # index -> character 변환\n",
    "    # => 사람이 읽을 수 있는 text 로 변경\n",
    "    def decode(self, encoded_label):\n",
    "        target_characters = list(self.character)\n",
    "        decoded_label = \"\"\n",
    "        for encode in encoded_label:\n",
    "            decoded_label += self.character[encode]\n",
    "        return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")  # 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)  # [ 8  5 12  0 12 15]\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)  # HEL-LO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "print(f'Train DataSet 개수: {len(train_set)}')  # 565\n",
    "print(f'Valid DataSet 개수: {len(val_set)}')  # 63\n",
    "print(f'Test DataSet 개수: {len(test_set)}')  # 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC loss를 계산하기 위한 Lambda 함수\n",
    "def ctc_lambda_func(args):\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "    \n",
    "    # Build CRNN model\n",
    "    # Conv Layers\n",
    "    # Feature 추출\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "\n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "\n",
    "    # Recurrent Layers\n",
    "    # 추출된 Feature 의 전체적인 Context 파악\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "\n",
    "    # Transcription Layer(=Fully Connected Layer)\n",
    "    # Step 마다 어떤 Character 의 확률이 높은지 예측\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    # CRNN 모델 생성\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(inputs=model_input, outputs=loss_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_crnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=f'result/Recognition_CRNN_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 컴파일 합니다\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.1, clipnorm=5)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'ctc': lambda y_true, y_pred: y_pred},\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 빨리 끝날 수 있도록 ModelCheckPoint와 EarlyStopping을 사용합니다\n",
    "checkpoint_path = os.path.join(DATA_DIR, f'model_checkpoint_{EPOCHS}.hdf5')\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min'\n",
    ")\n",
    "crnn_history = model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=len(train_set),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_set,\n",
    "    validation_steps=len(val_set),\n",
    "    callbacks=[ckp, earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_unet_history(history, model_name='Recognition_CRNN'):\n",
    "    plt.subplots(figsize=(12,4))\n",
    "\n",
    "    # Model Loss, Validation Loss 시각화\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], 'r')\n",
    "    plt.plot(history.history['val_loss'], 'b')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "\n",
    "    # Model Accuracy, Validation Accuracy 시각화\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'], 'r')\n",
    "    plt.plot(history.history['val_accuracy'], 'b')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'result/{model_name}_Result.png')  # 저장\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_unet_history(crnn_history, 'Recognition_CRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음은 20 Epochs 이상 학습된 모델의 가중치가 저장된 경로입니다\n",
    "checkpoint_path = os.path.join(DATA_DIR, f'model_checkpoint_{EPOCHS}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_crnn_model()\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crnn 모델은 입력이 복잡한 구조이므로 그대로 사용할 수가 없습니다\n",
    "# 그래서 crnn 모델의 입력중 'input_image' 부분만 사용한 모델을 새로 만들겁니다\n",
    "# inference 전용 모델이에요 \n",
    "input_data = model.get_layer('input_image').output\n",
    "y_pred = model.get_layer('output').output\n",
    "model_pred = Model(inputs=input_data, outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 inference한 결과를 글자로 바꿔주는 역할을 합니다\n",
    "# 코드 하나하나를 이해하기는 조금 어려울 수 있습니다\n",
    "def decode_predict_ctc(out, chars = TARGET_CHARACTERS):\n",
    "    results = []\n",
    "    indexes = K.get_value(\n",
    "        K.ctc_decode(\n",
    "            out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
    "            greedy=False , beam_width=5, top_paths=1\n",
    "        )[0][0]\n",
    "    )[0]\n",
    "    text = \"\"\n",
    "    for index in indexes:\n",
    "        # 의미없는 인덱스 -1 위치 숫자 9 출력 방지\n",
    "        if index == -1:\n",
    "            continue\n",
    "        # 예측한 문자 저장\n",
    "        text += chars[index]\n",
    "    return text\n",
    "\n",
    "# 모델과 데이터셋이 주어지면 inference를 수행합니다\n",
    "# index개 만큼의 데이터를 읽어 모델로 inference를 수행하고\n",
    "# 결과를 디코딩해 출력해줍니다\n",
    "def check_inference(model, dataset, index = 5):\n",
    "    for i in range(index):\n",
    "        print(f\"━━━━━━━━ {i} ━━━━━━━━\")\n",
    "        inputs, outputs = dataset[i]\n",
    "        img = inputs['input_image'][0:1,:,:,:]\n",
    "        output = model.predict(img)  # 모델 예측\n",
    "\n",
    "        # 예측\n",
    "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS).replace('-','')\n",
    "        print(f\"Result: \\t{result}\")\n",
    "        \n",
    "        # 라벨\n",
    "        label = ''.join(map(lambda x: label_converter.decode(x), inputs['label'][0:1,])).replace('-','')\n",
    "        print(f\"Label:  \\t{label}\")\n",
    "\n",
    "        # 예측, 라벨 인덱스별로 비교해서 같으면 1, 다르면 0 표시\n",
    "        is_right = ''.join(map(lambda x: '1' if x[0] == x[1] else '0', zip(result, label)))\n",
    "        print(f\"Is_Right:\\t{is_right}\")\n",
    "\n",
    "        # 맞춘 문자 개수, 점수 계산\n",
    "        is_right_count = is_right.count('1')\n",
    "        print(f\"Is_Right_Count:\\t{is_right_count}/{len(label)}\")\n",
    "        print(f\"Score: {is_right_count/len(label):.2f}\")\n",
    "\n",
    "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_inference(model_pred, test_set, index=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img_path, detector):\n",
    "    \n",
    "    # 이미지 불러오기\n",
    "    img_pil = Image.open(img_path).convert('RGB')  # (1200, 900, 3)\n",
    "\n",
    "    # 1. Keras-OCR 의 Detection 모델에 입력하기 위한 이미지 전처리\n",
    "    # - 이미지 비율 유지해서 resize() 적용\n",
    "    # - 배치 크기 때문에 4 dims 로 확장\n",
    "    # - shape=(H,W,C) 여야 모델 입력시 제대로 동작함\n",
    "    width, height = img_pil.size  # 원본 이미지 크기\n",
    "    target_width = min(int(width*300/height), 400)  # 이미지 비율 유지, 높이 300, 너비는 400 보다 작게 설정\n",
    "    target_img_size = (target_width,300)\n",
    "\n",
    "    img_pil = img_pil.resize(target_img_size)  # (400, 300, 3)\n",
    "    img_pil_copy = img_pil.copy()\n",
    "\n",
    "    img_pil = np.array(img_pil)  # (300, 400, 3)\n",
    "    img_pil = np.expand_dims(img_pil, axis=0)  # (1, 300, 400, 3)\n",
    "    \n",
    "    # 2. 모델 예측\n",
    "    # - OCR 텍스트 검출 결과\n",
    "    # - 배치 첫 번째 결과 가져오기\n",
    "    detect_result = detector.detect(img_pil)  # 타입: list\n",
    "    ocr_result = detect_result[0]  # 타입: np.array\n",
    "\t\n",
    "    # 3. display() 하기 위한 이미지 전처리\n",
    "    # - 앞서 4 dims 로 확장시켰으므로 다시 3 dims 가 되도록 축소\n",
    "    # - shape=(H,W,C) 여야 display() 에서 이미지가 제대로 그려짐\n",
    "    img_pil = np.squeeze(img_pil, axis=0).astype(np.uint8)  # (300, 400, 3)\n",
    "    img_pil = Image.fromarray(img_pil)\n",
    "    img_draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    cropped_imgs = []  # 단어 영역\n",
    "    for text_result in ocr_result:\n",
    "        img_draw.polygon(text_result, outline='red')\n",
    "        x_min = text_result[:,0].min() - 5\n",
    "        x_max = text_result[:,0].max() + 5\n",
    "        y_min = text_result[:,1].min() - 5\n",
    "        y_max = text_result[:,1].max() + 5\n",
    "        word_box = [x_min, y_min, x_max, y_max]\n",
    "        cropped_imgs.append(img_pil_copy.crop(word_box))\n",
    "    \n",
    "    return img_pil, cropped_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_img(idx, pil_img, model_pred, input_img_size=(100,32)):\n",
    "    # CRNN 기반의 Recognition 모델에 입력하기 위한 이미지 전처리\n",
    "    # - 배치 크기 때문에 4 dims 로 확장\n",
    "    # - shape=(W,H,C) 여야 모델 입력시 제대로 동작함\n",
    "    pil_img = pil_img.resize(input_img_size)  # (100, 32, 3)\n",
    "    pil_img = np.array(pil_img)  # (32, 100, 3)\n",
    "    pil_img = pil_img.transpose(1,0,2)  # (100, 32, 3)\n",
    "    pil_img = np.expand_dims(pil_img, axis=0)  # (1, 100, 32, 3)\n",
    "    \n",
    "    # 모델 예측\n",
    "    # - OCR 텍스트 인식 결과 출력\n",
    "    output = model_pred.predict(pil_img)\n",
    "    result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS).replace('-','')\n",
    "    print(\"Result: \\t\", result)\n",
    "    \n",
    "    # display() 하기 위한 이미지 전처리\n",
    "    # - 앞서 4 dims 로 확장시켰으므로 다시 3 dims 가 되도록 축소\n",
    "    # - shape=(H,W,C) 여야 display() 에서 이미지가 제대로 그려짐\n",
    "    pil_img = np.squeeze(pil_img, axis=0).transpose(1,0,2).astype(np.uint8)  # (32, 100, 3)\n",
    "    pil_img = Image.fromarray(pil_img)\n",
    "    pil_img.save(f'result/Text_Recognition_{idx}.png')  # 저장\n",
    "    display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_ocr(img_path, detector, model_pred):\n",
    "    # Text Detection\n",
    "    print('━━━━━━━━ Text Detection ━━━━━━━━')\n",
    "    img_pil, cropped_img = detect_text(img_path, detector)\n",
    "    img_pil.save(f'result/sample.png')  # 저장\n",
    "    display(img_pil)\n",
    "\n",
    "    # Text Recognition\n",
    "    print('━━━━━━━━ Text Recognition ━━━━━━━━')\n",
    "    for idx, _img in enumerate(cropped_img):\n",
    "        recognize_img(idx, _img, model_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IMG_PATH = os.path.join(DATA_DIR, 'sample.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = keras_ocr.detection.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_to_end_ocr(SAMPLE_IMG_PATH, detector, model_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 루브릭"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text recognition을 위해 특화된 데이터셋 구성이 체계적으로 진행되었다.\n",
    "\n",
    "텍스트 이미지 리사이징, ctc loss 측정을 위한 라벨 인코딩, 배치처리 등이 적절히 수행되었다.\n",
    "\n",
    "\n",
    "2. CRNN 기반의 recognition 모델의 학습이 정상적으로 진행되었다.\n",
    "\n",
    "학습결과 loss가 안정적으로 감소하고 대부분의 문자인식 추론 결과가 정확하다.\n",
    "\n",
    "\n",
    "3. keras-ocr detector와 CRNN recognizer를 엮어 원본 이미지 입력으로부터 text가 출력되는 OCR이 End-to-End로 구성되었다.\n",
    "\n",
    "샘플 이미지를 원본으로 받아 OCR 수행 결과를 리턴하는 1개의 함수가 만들어졌다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 회고"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "\n",
    "\n",
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**\n",
    "\n",
    "\n",
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**\n",
    "\n",
    "\n",
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**\n",
    "\n",
    "\n",
    "- **자기 다짐**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64f60db5d244171661447d9f571ca5a7e4bf0bab2148f20540b6d51562bb9ca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
